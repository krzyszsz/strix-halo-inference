[2026-02-15T18:18:24Z] watchdog start max_seconds=3600 idle_seconds=600 poll_seconds=30 watch_path=<none> min_available_gb=16 max_swap_used_gb=24 cmd=/home/kj/strix-halo-inference/scripts/run_test_with_cleanup.sh bash -lc set -euo pipefail; env OUT_DIR=/home/kj/strix-halo-inference/reports/quantize bash /home/kj/strix-halo-inference/llm-quantize/scripts/quantize_qwen25_05b_fp16_to_q4km.sh
[quantize] image=llama-cpp-tools:latest
[quantize] model_id=Qwen/Qwen2.5-0.5B-Instruct-GGUF
[quantize] fp16=/mnt/hf/models/qwen2.5-0.5b-instruct-gguf/qwen2.5-0.5b-instruct-fp16.gguf
[quantize] q4_local=/mnt/hf/models/qwen2.5-0.5b-instruct-gguf/qwen2.5-0.5b-instruct-q4_k_m-local.gguf
[quantize] out_dir=/home/kj/strix-halo-inference/reports/quantize
[container] fp16 already present, skipping download: /mnt/hf/models/qwen2.5-0.5b-instruct-gguf/qwen2.5-0.5b-instruct-fp16.gguf
[container] fp16_bytes=1266425696
[container] quantizing: FP16 -> Q4_K_M
main: build = 8022 (3bb78133a)
main: built with GNU 13.3.0 for Linux x86_64
main: quantizing '/mnt/hf/models/qwen2.5-0.5b-instruct-gguf/qwen2.5-0.5b-instruct-fp16.gguf' to '/mnt/hf/models/qwen2.5-0.5b-instruct-gguf/qwen2.5-0.5b-instruct-q4_k_m-local.gguf' as Q4_K_M
llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from /mnt/hf/models/qwen2.5-0.5b-instruct-gguf/qwen2.5-0.5b-instruct-fp16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = qwen2.5-0.5b-instruct
llama_model_loader: - kv   3:                            general.version str              = v0.1
llama_model_loader: - kv   4:                           general.finetune str              = qwen2.5-0.5b-instruct
llama_model_loader: - kv   5:                         general.size_label str              = 630M
llama_model_loader: - kv   6:                          qwen2.block_count u32              = 24
llama_model_loader: - kv   7:                       qwen2.context_length u32              = 8192
llama_model_loader: - kv   8:                     qwen2.embedding_length u32              = 896
llama_model_loader: - kv   9:                  qwen2.feed_forward_length u32              = 4864
llama_model_loader: - kv  10:                 qwen2.attention.head_count u32              = 14
llama_model_loader: - kv  11:              qwen2.attention.head_count_kv u32              = 2
llama_model_loader: - kv  12:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  13:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  14:                          general.file_type u32              = 1
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  121 tensors
llama_model_loader: - type  f16:  170 tensors
[   1/ 291]                        output.weight - [  896, 151936,     1,     1], type =    f16, converting to q8_0 .. size =   259.66 MiB ->   137.94 MiB
[   2/ 291]                   output_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[   3/ 291]                    token_embd.weight - [  896, 151936,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 151936 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =   259.66 MiB ->    89.26 MiB
[   4/ 291]                    blk.0.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[   5/ 291]                  blk.0.attn_k.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[   6/ 291]               blk.0.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[   7/ 291]             blk.0.attn_output.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[   8/ 291]                    blk.0.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[   9/ 291]                  blk.0.attn_q.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[  10/ 291]                    blk.0.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  11/ 291]                  blk.0.attn_v.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0
converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB
[  12/ 291]                blk.0.ffn_down.weight - [ 4864,   896,     1,     1], type =    f16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB
[  13/ 291]                blk.0.ffn_gate.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[  14/ 291]                blk.0.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[  15/ 291]                  blk.0.ffn_up.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[  16/ 291]                    blk.1.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  17/ 291]                  blk.1.attn_k.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[  18/ 291]               blk.1.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[  19/ 291]             blk.1.attn_output.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[  20/ 291]                    blk.1.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[  21/ 291]                  blk.1.attn_q.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[  22/ 291]                    blk.1.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  23/ 291]                  blk.1.attn_v.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0
converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB
[  24/ 291]                blk.1.ffn_down.weight - [ 4864,   896,     1,     1], type =    f16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB
[  25/ 291]                blk.1.ffn_gate.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[  26/ 291]                blk.1.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[  27/ 291]                  blk.1.ffn_up.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[  28/ 291]                    blk.2.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  29/ 291]                  blk.2.attn_k.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[  30/ 291]               blk.2.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[  31/ 291]             blk.2.attn_output.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[  32/ 291]                    blk.2.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[  33/ 291]                  blk.2.attn_q.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[  34/ 291]                    blk.2.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  35/ 291]                  blk.2.attn_v.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0
converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB
[  36/ 291]                blk.2.ffn_down.weight - [ 4864,   896,     1,     1], type =    f16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB
[  37/ 291]                blk.2.ffn_gate.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[  38/ 291]                blk.2.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[  39/ 291]                  blk.2.ffn_up.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[  40/ 291]                    blk.3.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  41/ 291]                  blk.3.attn_k.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[  42/ 291]               blk.3.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[  43/ 291]             blk.3.attn_output.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[  44/ 291]                    blk.3.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[  45/ 291]                  blk.3.attn_q.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[  46/ 291]                    blk.3.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  47/ 291]                  blk.3.attn_v.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[  48/ 291]                blk.3.ffn_down.weight - [ 4864,   896,     1,     1], type =    f16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB
[  49/ 291]                blk.3.ffn_gate.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[  50/ 291]                blk.3.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[  51/ 291]                  blk.3.ffn_up.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[  52/ 291]                    blk.4.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  53/ 291]                  blk.4.attn_k.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[  54/ 291]               blk.4.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[  55/ 291]             blk.4.attn_output.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[  56/ 291]                    blk.4.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[  57/ 291]                  blk.4.attn_q.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[  58/ 291]                    blk.4.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  59/ 291]                  blk.4.attn_v.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[  60/ 291]                blk.4.ffn_down.weight - [ 4864,   896,     1,     1], type =    f16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB
[  61/ 291]                blk.4.ffn_gate.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[  62/ 291]                blk.4.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[  63/ 291]                  blk.4.ffn_up.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[  64/ 291]                    blk.5.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  65/ 291]                  blk.5.attn_k.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[  66/ 291]               blk.5.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[  67/ 291]             blk.5.attn_output.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[  68/ 291]                    blk.5.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[  69/ 291]                  blk.5.attn_q.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[  70/ 291]                    blk.5.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  71/ 291]                  blk.5.attn_v.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0
converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB
[  72/ 291]                blk.5.ffn_down.weight - [ 4864,   896,     1,     1], type =    f16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB
[  73/ 291]                blk.5.ffn_gate.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[  74/ 291]                blk.5.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[  75/ 291]                  blk.5.ffn_up.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[  76/ 291]                    blk.6.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  77/ 291]                  blk.6.attn_k.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[  78/ 291]               blk.6.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[  79/ 291]             blk.6.attn_output.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[  80/ 291]                    blk.6.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[  81/ 291]                  blk.6.attn_q.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[  82/ 291]                    blk.6.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  83/ 291]                  blk.6.attn_v.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[  84/ 291]                blk.6.ffn_down.weight - [ 4864,   896,     1,     1], type =    f16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB
[  85/ 291]                blk.6.ffn_gate.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[  86/ 291]                blk.6.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[  87/ 291]                  blk.6.ffn_up.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[  88/ 291]                    blk.7.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  89/ 291]                  blk.7.attn_k.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[  90/ 291]               blk.7.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[  91/ 291]             blk.7.attn_output.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[  92/ 291]                    blk.7.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[  93/ 291]                  blk.7.attn_q.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[  94/ 291]                    blk.7.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[  95/ 291]                  blk.7.attn_v.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[  96/ 291]                blk.7.ffn_down.weight - [ 4864,   896,     1,     1], type =    f16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB
[  97/ 291]                blk.7.ffn_gate.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[  98/ 291]                blk.7.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[  99/ 291]                  blk.7.ffn_up.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 100/ 291]                    blk.8.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 101/ 291]                  blk.8.attn_k.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[ 102/ 291]               blk.8.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 103/ 291]             blk.8.attn_output.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 104/ 291]                    blk.8.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 105/ 291]                  blk.8.attn_q.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 106/ 291]                    blk.8.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 107/ 291]                  blk.8.attn_v.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0
converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB
[ 108/ 291]                blk.8.ffn_down.weight - [ 4864,   896,     1,     1], type =    f16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB
[ 109/ 291]                blk.8.ffn_gate.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 110/ 291]                blk.8.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 111/ 291]                  blk.8.ffn_up.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 112/ 291]                    blk.9.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 113/ 291]                  blk.9.attn_k.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[ 114/ 291]               blk.9.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 115/ 291]             blk.9.attn_output.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 116/ 291]                    blk.9.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 117/ 291]                  blk.9.attn_q.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 118/ 291]                    blk.9.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 119/ 291]                  blk.9.attn_v.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[ 120/ 291]                blk.9.ffn_down.weight - [ 4864,   896,     1,     1], type =    f16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB
[ 121/ 291]                blk.9.ffn_gate.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 122/ 291]                blk.9.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 123/ 291]                  blk.9.ffn_up.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 124/ 291]                   blk.10.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 125/ 291]                 blk.10.attn_k.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[ 126/ 291]              blk.10.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 127/ 291]            blk.10.attn_output.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 128/ 291]                   blk.10.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 129/ 291]                 blk.10.attn_q.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 130/ 291]                   blk.10.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 131/ 291]                 blk.10.attn_v.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[ 132/ 291]               blk.10.ffn_down.weight - [ 4864,   896,     1,     1], type =    f16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB
[ 133/ 291]               blk.10.ffn_gate.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 134/ 291]               blk.10.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 135/ 291]                 blk.10.ffn_up.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 136/ 291]                   blk.11.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 137/ 291]                 blk.11.attn_k.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[ 138/ 291]              blk.11.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 139/ 291]            blk.11.attn_output.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 140/ 291]                   blk.11.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 141/ 291]                 blk.11.attn_q.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 142/ 291]                   blk.11.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 143/ 291]                 blk.11.attn_v.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0
converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB
[ 144/ 291]               blk.11.ffn_down.weight - [ 4864,   896,     1,     1], type =    f16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB
[ 145/ 291]               blk.11.ffn_gate.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 146/ 291]               blk.11.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 147/ 291]                 blk.11.ffn_up.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 148/ 291]                   blk.12.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 149/ 291]                 blk.12.attn_k.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[ 150/ 291]              blk.12.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 151/ 291]            blk.12.attn_output.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 152/ 291]                   blk.12.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 153/ 291]                 blk.12.attn_q.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 154/ 291]                   blk.12.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 155/ 291]                 blk.12.attn_v.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[ 156/ 291]               blk.12.ffn_down.weight - [ 4864,   896,     1,     1], type =    f16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB
[ 157/ 291]               blk.12.ffn_gate.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 158/ 291]               blk.12.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 159/ 291]                 blk.12.ffn_up.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 160/ 291]                   blk.13.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 161/ 291]                 blk.13.attn_k.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[ 162/ 291]              blk.13.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 163/ 291]            blk.13.attn_output.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 164/ 291]                   blk.13.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 165/ 291]                 blk.13.attn_q.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 166/ 291]                   blk.13.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 167/ 291]                 blk.13.attn_v.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[ 168/ 291]               blk.13.ffn_down.weight - [ 4864,   896,     1,     1], type =    f16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB
[ 169/ 291]               blk.13.ffn_gate.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 170/ 291]               blk.13.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 171/ 291]                 blk.13.ffn_up.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 172/ 291]                   blk.14.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 173/ 291]                 blk.14.attn_k.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[ 174/ 291]              blk.14.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 175/ 291]            blk.14.attn_output.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 176/ 291]                   blk.14.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 177/ 291]                 blk.14.attn_q.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 178/ 291]                   blk.14.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 179/ 291]                 blk.14.attn_v.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0
converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB
[ 180/ 291]               blk.14.ffn_down.weight - [ 4864,   896,     1,     1], type =    f16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB
[ 181/ 291]               blk.14.ffn_gate.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 182/ 291]               blk.14.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 183/ 291]                 blk.14.ffn_up.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 184/ 291]                   blk.15.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 185/ 291]                 blk.15.attn_k.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[ 186/ 291]              blk.15.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 187/ 291]            blk.15.attn_output.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 188/ 291]                   blk.15.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 189/ 291]                 blk.15.attn_q.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 190/ 291]                   blk.15.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 191/ 291]                 blk.15.attn_v.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[ 192/ 291]               blk.15.ffn_down.weight - [ 4864,   896,     1,     1], type =    f16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB
[ 193/ 291]               blk.15.ffn_gate.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 194/ 291]               blk.15.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 195/ 291]                 blk.15.ffn_up.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 196/ 291]                   blk.16.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 197/ 291]                 blk.16.attn_k.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[ 198/ 291]              blk.16.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 199/ 291]            blk.16.attn_output.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 200/ 291]                   blk.16.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 201/ 291]                 blk.16.attn_q.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 202/ 291]                   blk.16.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 203/ 291]                 blk.16.attn_v.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[ 204/ 291]               blk.16.ffn_down.weight - [ 4864,   896,     1,     1], type =    f16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB
[ 205/ 291]               blk.16.ffn_gate.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 206/ 291]               blk.16.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 207/ 291]                 blk.16.ffn_up.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 208/ 291]                   blk.17.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 209/ 291]                 blk.17.attn_k.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[ 210/ 291]              blk.17.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 211/ 291]            blk.17.attn_output.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 212/ 291]                   blk.17.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 213/ 291]                 blk.17.attn_q.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 214/ 291]                   blk.17.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 215/ 291]                 blk.17.attn_v.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0
converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB
[ 216/ 291]               blk.17.ffn_down.weight - [ 4864,   896,     1,     1], type =    f16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB
[ 217/ 291]               blk.17.ffn_gate.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 218/ 291]               blk.17.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 219/ 291]                 blk.17.ffn_up.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 220/ 291]                   blk.18.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 221/ 291]                 blk.18.attn_k.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[ 222/ 291]              blk.18.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 223/ 291]            blk.18.attn_output.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 224/ 291]                   blk.18.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 225/ 291]                 blk.18.attn_q.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 226/ 291]                   blk.18.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 227/ 291]                 blk.18.attn_v.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[ 228/ 291]               blk.18.ffn_down.weight - [ 4864,   896,     1,     1], type =    f16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB
[ 229/ 291]               blk.18.ffn_gate.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 230/ 291]               blk.18.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 231/ 291]                 blk.18.ffn_up.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 232/ 291]                   blk.19.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 233/ 291]                 blk.19.attn_k.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[ 234/ 291]              blk.19.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 235/ 291]            blk.19.attn_output.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 236/ 291]                   blk.19.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 237/ 291]                 blk.19.attn_q.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 238/ 291]                   blk.19.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 239/ 291]                 blk.19.attn_v.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[ 240/ 291]               blk.19.ffn_down.weight - [ 4864,   896,     1,     1], type =    f16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB
[ 241/ 291]               blk.19.ffn_gate.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 242/ 291]               blk.19.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 243/ 291]                 blk.19.ffn_up.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 244/ 291]                   blk.20.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 245/ 291]                 blk.20.attn_k.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[ 246/ 291]              blk.20.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 247/ 291]            blk.20.attn_output.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 248/ 291]                   blk.20.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 249/ 291]                 blk.20.attn_q.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 250/ 291]                   blk.20.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 251/ 291]                 blk.20.attn_v.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0
converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB
[ 252/ 291]               blk.20.ffn_down.weight - [ 4864,   896,     1,     1], type =    f16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB
[ 253/ 291]               blk.20.ffn_gate.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 254/ 291]               blk.20.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 255/ 291]                 blk.20.ffn_up.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 256/ 291]                   blk.21.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 257/ 291]                 blk.21.attn_k.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[ 258/ 291]              blk.21.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 259/ 291]            blk.21.attn_output.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 260/ 291]                   blk.21.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 261/ 291]                 blk.21.attn_q.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 262/ 291]                   blk.21.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 263/ 291]                 blk.21.attn_v.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0
converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB
[ 264/ 291]               blk.21.ffn_down.weight - [ 4864,   896,     1,     1], type =    f16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB
[ 265/ 291]               blk.21.ffn_gate.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 266/ 291]               blk.21.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 267/ 291]                 blk.21.ffn_up.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 268/ 291]                   blk.22.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 269/ 291]                 blk.22.attn_k.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[ 270/ 291]              blk.22.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 271/ 291]            blk.22.attn_output.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 272/ 291]                   blk.22.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 273/ 291]                 blk.22.attn_q.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 274/ 291]                   blk.22.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 275/ 291]                 blk.22.attn_v.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0
converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB
[ 276/ 291]               blk.22.ffn_down.weight - [ 4864,   896,     1,     1], type =    f16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB
[ 277/ 291]               blk.22.ffn_gate.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 278/ 291]               blk.22.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 279/ 291]                 blk.22.ffn_up.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 280/ 291]                   blk.23.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 281/ 291]                 blk.23.attn_k.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB
[ 282/ 291]              blk.23.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 283/ 291]            blk.23.attn_output.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 284/ 291]                   blk.23.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 285/ 291]                 blk.23.attn_q.weight - [  896,   896,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB
[ 286/ 291]                   blk.23.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB
[ 287/ 291]                 blk.23.attn_v.weight - [  896,   128,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0
converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB
[ 288/ 291]               blk.23.ffn_down.weight - [ 4864,   896,     1,     1], type =    f16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB
[ 289/ 291]               blk.23.ffn_gate.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
[ 290/ 291]               blk.23.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MiB
[ 291/ 291]                 blk.23.ffn_up.weight - [  896,  4864,     1,     1], type =    f16, 

llama_model_quantize_impl : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0
converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB
llama_model_quantize_impl: model size  =  1202.09 MiB
llama_model_quantize_impl: quant size  =   462.96 MiB
llama_model_quantize_impl: WARNING: 145 of 170 tensor(s) required fallback quantization

main: quantize time =  2584.52 ms
main:    total time =  2584.52 ms
[container] q4_bytes=491400032
[container] inference (fp16)

Loading model... |-\| 


▄▄ ▄▄
██ ██
██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄
██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██
██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀
                                    ██    ██
                                    ▀▀    ▀▀

build      : b8022-3bb78133a
model      : qwen2.5-0.5b-instruct-fp16.gguf
modalities : text

available commands:
  /exit or Ctrl+C     stop or exit
  /regen              regenerate the last response
  /clear              clear the chat history
  /read               add a text file


> Write a 4-line poem about Vulkan on AMD Strix Halo.

| In the AMD Strix Halo, the Vulkan engine shines,
A powerful platform that's set to soar.
With AMD's expertise and Strix's design,
Vulkan's journey is destined to be grand.
A testament to the power of the AMD Strix Halo,  
A future where the best is here to stay.llama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute    unaccounted |
llama_memory_breakdown_print: |   - Host               |                 1619 =  1202 +      96 +     321                |


[ Prompt: 1399.3 t/s | Generation: 66.9 t/s ]

Exiting...
[container] inference (q4)

Loading model... |-\| 


▄▄ ▄▄
██ ██
██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄
██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██
██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀
                                    ██    ██
                                    ▀▀    ▀▀

build      : b8022-3bb78133a
model      : qwen2.5-0.5b-instruct-q4_k_m-local.gguf
modalities : text

available commands:
  /exit or Ctrl+C     stop or exit
  /regen              regenerate the last response
  /clear              clear the chat history
  /read               add a text file


> Write a 4-line poem about Vulkan on AMD Strix Halo.

| In the realm of Vulkan, AMD Strix Halo shines,
A technology so pure and so true.
Its memory is vast and its power so strong,
On AMD Strix Halo, you can go where you will.llama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute    unaccounted |
llama_memory_breakdown_print: |   - Host               |                  880 =   462 +      96 +     321                |
llama_memory_breakdown_print: |   - CPU_REPACK         |                   28 =    28 +       0 +       0                |


[ Prompt: 985.1 t/s | Generation: 136.0 t/s ]

Exiting...
[container] wrote /home/kj/strix-halo-inference/reports/quantize/quantize_outputs.json
[quantize] done
[2026-02-15T18:18:54Z] watchdog finished cmd_status=0
