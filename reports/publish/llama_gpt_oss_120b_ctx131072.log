[2026-02-15T16:44:58Z] watchdog start max_seconds=3600 idle_seconds=600 poll_seconds=30 watch_path=<none> min_available_gb=16 max_swap_used_gb=24 cmd=/home/kj/strix-halo-inference/scripts/run_test_with_cleanup.sh bash -lc set -euo pipefail; env MODEL_PATH=/mnt/hf/models/gpt-oss-120b-gguf/gpt-oss-120b-mxfp4-00001-of-00003.gguf PORT=8115 CTX_SIZE=131072 MAX_TOKENS=1024 PROMPT_MODE=text THREADS=8 GPU_LAYERS=999 OUT_JSON=/home/kj/strix-halo-inference/llama-cpp-vulkan/out/gpt_oss_120b_mxfp4_ctx_131072_75g_retest2.json bash /home/kj/strix-halo-inference/llama-cpp-vulkan/scripts/probe_ctx_once.sh
[2026-02-15T16:44:58Z] Starting container=llama-ctx-probe-8115 ctx=131072 max_tokens=1024 mem=75g swap=75g reservation=67g
[2026-02-15T16:44:59Z] Container started id=3d69460809f8563fdcc6094489a3d43a11d0e6efd6f2db40f2bea9d323a4892c
[2026-02-15T16:45:24Z] Waiting for server readiness on port=8115 attempt=6/180 http=503
[2026-02-15T16:45:55Z] Waiting for server readiness on port=8115 attempt=12/180 http=503
[2026-02-15T16:46:26Z] Server ready on port=8115; submitting chat request (curl_max_time=1200s)
[2026-02-15T16:46:56Z] Chat request still running (elapsed=30s ctx=131072)
[2026-02-15T16:46:56Z] Chat request completed successfully
/home/kj/strix-halo-inference/llama-cpp-vulkan/out/gpt_oss_120b_mxfp4_ctx_131072_75g_retest2.json bytes=3056
[2026-02-15T16:46:57Z] Probe completed and artifact saved: /home/kj/strix-halo-inference/llama-cpp-vulkan/out/gpt_oss_120b_mxfp4_ctx_131072_75g_retest2.json
CTX probe success: model=/mnt/hf/models/gpt-oss-120b-gguf/gpt-oss-120b-mxfp4-00001-of-00003.gguf ctx=131072 max_tokens=1024 out=/home/kj/strix-halo-inference/llama-cpp-vulkan/out/gpt_oss_120b_mxfp4_ctx_131072_75g_retest2.json
[2026-02-15T16:47:28Z] watchdog finished cmd_status=0
